## Chapter 1 Who Are You?

Suppose someone asked, “Who are you?” It would be simple enough to respond with your name. But if the person wanted to know the entire story about who you are, the question would be more difficult to answer. You’d obviously have to give the details of your height, age, and weight. You’d also have to include all your sentiments and preferences, even the secret ones you’ve never shared with anyone—your affection for your loved ones; your desire to please the people you associate with; your dislike of your older sister’s husband; your allegiance to your favorite beverage, brand of clothing, and music.

Your attitudes couldn’t be overlooked either—your impatience when an issue gets complex, your aversion to certain courses, your fear of high places and dogs and speaking in public. The list would go on. To be complete, it would have to include all your characteristics—not only the physical but also the emotional and intellectual.

To provide all that information would be quite a chore. But suppose the questioner was still curious and asked, “How did you get the way you are?” If your patience were not yet exhausted, chances are you’d answer something like this: “I’m this way because I choose to be, because I’ve considered other sentiments and preferences and attitudes and have made my selections. The ones I have chosen fit my style and personality best.” That answer is natural enough, and in part it’s true. But in a larger sense, it’s not true. The impact of the world on all of us is much greater than most of us realize.

### The Influence of Time and Place

Not only are you a member of a particular species, _Homo sapiens_ , but you also exist at a particular time in the history of that species and in a particular place on the planet. That time and place are defined by specific circumstances, understandings, beliefs, and customs, all of which limit your experience and influence your thought patterns. If you had lived in America in colonial times, you likely would have had no objection to the practice of barring women from serving on a jury, entering into a legal contract, owning property, or voting. If you had lived in the nineteenth century, you would have had no objection to young children being denied an education and being hired out by their parents to work sixteen hours a day, nor would you have given any thought to the special needs of adolescence. (The concept of adolescence was not invented until 1904.)

If you had been raised in the Middle East, you would stand much closer to people you converse with than you do in America. If you had been raised in India, you might be perfectly comfortable having your parents choose your spouse for you. If your native language were Spanish and your knowledge of English modest, you probably would be confused by some English colloquialisms. James Henslin offers two amusing examples of such confusion: Chevrolet Novas initially sold very poorly in Mexico because _no va_ in Spanish means “it doesn’t work”; and Perdue chickens were regarded with a certain suspicion (or worse) because the company’s slogan—”It takes a tough man to make a tender chicken”— became in Spanish “It takes an aroused man to make a chicken affectionate.”

People who grow up in Europe, Asia, or South America have very different ideas of punctuality. As Daniel Goleman explains, “Five minutes is late but permissible for a business appointment in the U.S., but thirty minutes is normal in Arab countries. In England five to fifteen minutes is the ‘correct’ lateness for one invited to dinner; an Italian might come two hours late, an Ethiopian still later, a Javanese not at all, having accepted only to prevent his host’s losing face.” A different ethnic origin would also mean different tastes in food. Instead of craving a New York Strip steak and french fries, you might crave “raw monkey brains” or “camel’s milk cheese patties cured in dry camel’s dung” and washed down with “warm camel’s blood.” Sociologist Ian Robertson summed up the range of global dietary differences succinctly: “Americans eat oysters but not snails. The French eat snails but not locusts. The Zulus eat locusts but not fish. The Jews eat fish but not pork. The Hindus eat pork but not beef. The Russians eat beef but not snakes. The Chinese eat snakes but not people. The Jalé of New Guinea find people delicious.” [Note: The reference to Hindus is mistaken.]

To sum up, living in a different age or culture would make you a different person. Even if you rebelled against the values of your time and place, they still would represent the context of your life—in other words, they still would influence your responses.

### The Influence of Ideas

When one idea is expressed, closely related ideas are simultaneously conveyed, logically and inescapably. In logic, this kinship is expressed by the term _sequitur,_ Latin for “it follows.” (The converse is _non sequitur,_ “it does not follow.”)

Consider, for example, the idea that many teachers and parents express to young children as a way of encouraging them: “If you believe in your self, you can succeed at anything.” From this it follows that nothing else but belief—neither talent nor hard work—is necessary for success. The reason the two ideas are equivalent is that their meanings are inseparably linked.*

In addition to conveying ideas closely linked to it in meaning, an idea can imply other ideas. For example, the idea that there is no real difference between virtue and vice implies that people should not feel bound by common moral standards. Samuel Johnson had this implication in mind when he said: “But if he does really think that there is no distinction between virtue and vice, why, Sir, when he leaves our houses let us count our spoons.”

If we were fully aware of the closely linked meanings and implications of the ideas we encounter, we could easily sort out the sound ones from the unsound, the wise from the foolish, and the helpful from the harmful. But we are seldom fully aware. In many cases, we take ideas at face value and embrace them with little or no thought of their associated meanings and implications. In the course of time, our actions are shaped by those meanings and implications, whether we are aware of them or not.

To appreciate the influence of ideas in people’s lives, consider the series of events set in motion by an idea that was popular in psychology more than a century ago and whose influence continues to this day—the idea that “intelligence is genetically determined and cannot be increased.”

> That idea led researchers to devise tests that measure intelligence. The most famous (badly flawed) test determined that the average mental age of white American adults was 13 and that, among immigrants, the average Russian’s mental age was 11.34; the average Italian’s, 11.01; the average Pole’s, 10.74; and the average mental age of “Negroes,” 10.41.
>
> Educators read the text results and thought, “Attempts to raise students’ intelligence are pointless,” so they replaced academic curricula with vocational curricula and embraced a methodology that taught students facts but not the process of judgment.
>
> Legislators read the test results and decided “We’ve got to do something to keep intellectually inferior people from entering the country,” so they revised immigration laws to discriminate against southern and central Europeans.
>
> Eugenicists, who had long been concerned about the welfare of the human species, saw the tests as a grave warning. They thought, “If intelligence cannot be increased, we must find ways of encouraging reproduction among people of higher intelligence and discouraging it among those of lower intelligence.”
>
> The eugenicists’ concern inspired a variety of actions. Margaret Sanger’s Planned Parenthood urged the lower classes to practice contraception. Others succeeded in legalizing promoted forced sterilization, notably in Virginia. The U.S. Supreme Court upheld the Virginia law with Justice Oliver Wendell Holmes, Jr. declaring, “Three generations of imbeciles are enough.” Over the next five decades 7,500 women, including “unwed mothers, prostitutes, petty criminals and children with disciplinary problems” were sterilized. In addition, by 1950 over 150,000 supposedly “defective” children, many relatively normal, were held against their will in institutions. They “endured isolation, overcrowding, forced labor, and physical abuse including lobotomy, electroshock, and surgical sterilization.”
>
> Meanwhile, business leaders read the test results and decided, “ _We need policies to ensure that workers leave their minds at the factory gate and perform their assigned tasks mindlessly_ .” So they enacted those policies. Decades later, when Edwards Deming proposed his “quality control” ideas for involving workers in decision making, business leaders remembered those test results and ignored Deming’s advice. (In contrast, the Japanese welcomed Deming’s ideas; as a result, several of their industries surged ahead of their American competition.)

These are the most obvious effects of hereditarianism but they are certainly not the only ones. Others include discrimination against racial and ethnic minorities and the often-paternalistic policies of government offered in response. (Some historians also link hereditarianism to the genocide that occurred in Nazi Germany.)

The innumerable ideas you have encountered will affect your beliefs and behavior in similar ways––sometimes slightly, at other times profoundly. And this can happen even if you have not consciously embraced the ideas.

### The Influence of Mass Culture

In centuries past, family and teachers were the dominant, and sometimes the only, influence on children. Today, however, the influence exerted by mass culture (the broadcast media, newspapers, magazines, Internet and popular music) often is greater.

By age 18 the average teenager has spent 11,000 hours in the classroom and 22,000 hours in front of the television set. He or she has had perhaps 13,000 school lessons yet has watched more than 750,000 commercials. By age thirty-five the same person has had fewer than 20,000 school lessons yet has watched approximately 45,000 hours of television and close to 2 million commercials.

What effects does mass culture have on us? To answer, we need only consider the formats and devices commonly used in the media. Modern advertising typically bombards the public with slogans and testimonials by celebrities. This approach is designed to appeal to emotions and create artificial needs for products and services. As a result, many people develop the habit of responding emotionally, impulsively, and gullibly to such appeals. They also tend to acquire values very different from those taught in the home and the school. Ads often portray play as more fulfilling than work, self-gratification as more desirable than self-control, and materialism as more meaningful than idealism.

Television programmers use frequent scene shifts and sensory appeals such as car crashes, violence, and sexual encounters to keep audience interest from diminishing. Then they add frequent commercial interruptions. This author has analyzed the attention shifts that television viewers are subjected to. In a dramatic program, for example, attention shifts might include camera angle changes;* shifts in story line from one set of characters (or subplot) to another, or from a present scene to a past scene (flashback), or to fantasy; and shifts to “newsbreaks,” to commercial breaks, from one commercial to another, and back to the program. Also included might be shifts of attention that occur within commercials. I found as many as 78 shifts per hour, excluding the shifts within commercials. The number of shifts within commercials ranged from 6 to 54 and aver aged approximately 17 per fifteen-second commercial. The total number of attention shifts came out to over 800 per hour, or over 14 per minute.†

This manipulation has prevented many people from developing a mature attention span. They expect the classroom and the workplace to provide the same constant excitement they get from television. That, of course, is an impossible demand, and when it isn’t met they call their teachers boring and their work unfulfilling. Because such people seldom have the patience to read books that require them to think, many publishers have replaced serious books with light fare written by celebrities.

Even when writers of serious books do manage to become published authors, they are often directed to give short, dramatic answers during promotional interviews, sometimes at the expense of accuracy. A man who coaches writers for talk shows offered one client this advice: “If I ask you whether the budget deficit is a good thing or a bad thing, you should not say, ‘Well, it stimulates the economy but it passes on a burden.’ You have to say ‘It’s a great idea!’ or ‘It’s a terrible idea!’ It doesn’t matter which.” ( _Translation_ : ”Don’t give a balanced answer. Give an oversimplified one because it will get you noticed.”)

Print journalism is also in the grip of sensationalism. As a newspaper editor observed, “Journalists keep trying to find people who are at 1 or at 9 on a scale of 1 to 10 rather than people at 3 to 7 [the more moderate positions] where most people actually are.”3 Another journalist claims, “News is now becoming more opinion than verified fact. Journalists are slipping into entertainment rather than telling us the verified facts we need to know.”

Today’s politicians often manipulate people more offensively than do journalists. Instead of expressing their thoughts, some politicians find out what people think and pretend to share their ideas. Many politicians hire people to conduct polls and focus groups to learn what messages will “sell.” They even go so far as to test the impact of certain words—that is why we hear so much about “trust,” “family,” “character,” and “values” these days. Political science professor Larry Sabato says that during the Clinton impeachment trial, the president’s advisors used the term _private lives_ over and over—James Carville used it six times in one four-minute speech—because they knew it could persuade people into believing the president’s lying under oath was of no great consequence.

### The “Science” of Manipulation

Attempts to influence the thoughts and actions of others are no doubt as old as time, but manipulation did not become a science until the early twentieth century, when Ivan Pavlov, a Russian professor of psychology, published his research on conditioned (learned) reflexes. Pavlov found that by ringing a bell when he fed a dog, he could condition the dog to drool at the sound of the bell even when no food was presented. An American psychologist, John Watson, was impressed with Pavlov’s findings and applied them to human behavior. In Watson’s most famous experiment, he let a baby touch a laboratory rat. At first, the baby was unafraid. But then Watson hit a hammer against metal whenever the baby reached out to touch the rat, and the baby became frightened and cried. In time, the baby cried not only at the sight of the rat but also at the sight of anything furry, such as a stuffed animal. Watson’s work earned him the title “father of behaviorism.”

Less well known is Watson’s application of behaviorist principles to advertising. He spent the latter part of his career working for advertising agencies and soon recognized that the most effective appeal to consumers was not to the mind but to the emotions. He advised advertisers to “tell [the consumer] something that will tie him up with fear, something that will stir up a mild rage, that will call out an affectionate or love response, or strike at a deep psychological or habit need.” His attitude toward the consumer is perhaps best indicated by a statement he made in a presentation to department store executives: “The consumer is to the manufacturer, the department stores and the advertising agencies, what the green frog is to the physiologist.”

Watson introduced these strategies in the 1920s and 1930s, the age of newspapers and radio. Since the advent of television, these advertising strategies have grown more sophisticated and effective, so much so that many individuals and groups with political and social agendas have adopted them. The strategies work for a number of reasons, the chief one being people’s conviction that they are impervious to manipulation. This belief is mistaken, as many researchers have demonstrated. For example, Solomon Asch showed that people’s reactions can be altered simply by changing the order of words in a series. He asked study participants to evaluate a person by a series of adjectives. When he put positive adjectives first—”intelligent, industrious, impulsive, critical, stubborn, envious”— the participants gave a positive evaluation. When he reversed the order, with “envious” coming first and “intelligent” last, they gave a negative evaluation.

Similarly, research has shown that human memory can be manipulated. The way a question is asked can change the details in a person’s memory and even make a person _remember something that never happened_!

Of course, advertisers and people with political or social agendas are not content to stimulate emotions and/or plant ideas in our minds. They also seek to reinforce those impressions by repeating them again and again. The more people hear a slogan or talking point, the more familiar it becomes. Before long, it becomes indistinguishable from ideas developed through careful thought. Sadly, “the packaging is often done so effectively that the viewer, listener, or reader does not make up his own mind at all. Instead, he inserts a packaged opinion into his mind, somewhat like inserting a DVD into a DVD player. He then pushes a button and ‘plays back’ the opinion whenever it seems appropriate to do so. He has performed acceptably without having had to think.” Many of the beliefs we hold dearest and defend most vigorously may have been planted in our minds in just this way.

Many years ago, Harry A. Overstreet noted that “a climate of opinion, like a physical climate, is so pervasive a thing that those who live within it and know no other take it for granted.” The rise of mass culture and the sophisticated use of manipulation have made this insight more relevant today than ever.

### The Influence of Psychology

The social and psychological theories of our time also have an impact on our beliefs. Before the past few decades, people were urged to be self disciplined, self-critical, and self-effacing. They were urged to practice self denial, to aspire to self-knowledge, to behave in a manner that ensured they maintained self-respect. Self-centeredness was considered a vice. “Hard work,” they were told, “leads to achievement, and that in turn produces satisfaction and self-confidence.” By and large, our grandparents internalized those teachings. When they honored them in their behavior, they felt proud; when they dishonored them, they felt ashamed.

Today the theories have been changed—indeed, almost exactly reversed. Self-esteem, which nineteenth-century satirist Ambrose Bierce defined as “an erroneous appraisement,” is now considered an imperative. Self-centeredness has been transformed from vice into virtue, and people who devote their lives to helping others, people once considered heroic and saintlike, are now said to be afflicted with “a disease to please.” The formula for success and happiness begins with feeling good about ourselves. Students who do poorly in school, workers who don’t measure up to the challenges of their jobs, substance abusers, lawbreakers—all are typically diagnosed as deficient in self-esteem.

In addition, just as our grandparents internalized the social and psychological theories of their time, so most contemporary Americans have internalized the message of self-esteem. We hear people speak of it over coffee; we hear it endlessly invoked on talk shows. Challenges to its precepts are usually met with disapproval.

But isn’t the theory of self-esteem self-evident? No. A negative perception of our abilities will, of course, handicap our performance. Dr. Maxwell Maltz explains the amazing results one educator had in improving the grades of schoolchildren by changing their self-images. The educator had observed that when the children saw themselves as stupid in a particular subject (or stupid in general), they unconsciously acted to confirm their self-images. They believed they were stupid, so they acted that way. Reasoning that it was their defeatist attitude rather than any lack of ability that was undermining their efforts, the educator set out to change their self-images. He found that when he accomplished that, _they no longer behaved stupidly_! Maltz concludes from this and other examples that our experiences can work a kind of self-hypnotism on us, suggesting a conclusion about ourselves and then urging us to make it come true.

Many proponents of self-esteem went far beyond Maltz’s demonstration that self-confidence is an important ingredient in success. They claimed that there is no such thing as too much self-esteem. Research does not support that claim. For example, Martin Seligman, an eminent research psychologist and founder of the movement known as positive psychology, cites significant evidence that, rather than _solving_ personal and social problems, including depression, the modern emphasis on self-esteem _causes_ them.

Maltz’s research documents that lack of confidence impedes performance, a valuable insight. But such research doesn’t explain why the more global concept of self-esteem has become so dominant. The answer to that question lies in the popularization of the work of humanistic psychologists such as Abraham Maslow. Maslow described what he called the hierarchy of human needs in the form of a pyramid, with physiological needs (food and drink) at the foundation. Above them, in ascending order, are safety needs, the need for belongingness and love, the need for esteem and approval, and aesthetic and cognitive needs (knowledge, understanding, etc.). At the pinnacle is the need for self-actualization, or fulfillment of our potential. In Maslow’s view, the lower needs must be fulfilled before the higher ones. It’s easy to see how the idea that self esteem must precede achievement was derived from Maslow’s theory.

Other theories might have been adopted, however. A notable one is Austrian psychiatrist Viktor Frankl’s, which was advanced at roughly the same time as Maslow’s and was based on both Frankl’s professional practice and his experiences in Hitler’s concentration camps. Frankl argues that one human need is higher than self-actualization: _self-transcendence,_ the need to rise above narrow absorption with self. According to Frankl, “the primordial anthropological fact [is] that being human is being always directed, and pointing to something or someone other than oneself: to a meaning to fulfill or another human being to encounter, a cause to serve or a person to love.” A person becomes fully human “by forgetting himself and giving himself, overlooking himself and focusing outward.”

Making self-actualization (or happiness) the direct object of our pursuit, in Frankl’s view, is ultimately self-defeating; such fulfillment can occur only as “the unintended effect of self-transcendence.” The proper perspective on life, Frankl believes, is not what it can give _to_ us, but what it expects _from_ us; life is daily—even hourly—questioning us, challenging us to accept “the responsibility to find the right answer to its problems and to fulfill the tasks which it constantly sets for [each of us].”

Finding meaning, according to Frankl’s theory, involves “perceiving a possibility embedded in reality” and searching for challenging tasks “whose completion might add meaning to [one’s] existence.” But such perceiving and searching are frustrated by the focus on self: “As long as modern literature confines itself to, and contents itself with, self-expression—not to say self-exhibition—it reflects its authors’ sense of futility and absurdity. What is more important, it also creates absurdity. This is understandable in light of the fact that meaning must be discovered, it cannot be invented. Sense cannot be created, but what may well be created is nonsense.”

Whether we agree completely with Frankl, one thing is clear: Contemporary American culture would be markedly different if the emphasis over the past several decades had been on Frankl’s theory rather than on the theories of Maslow and the other humanistic psychologists. All of us would have been affected—we can only imagine how profoundly—in our attitudes, values, and beliefs.

### Becoming an Individual

In light of what we have discussed, we should regard individuality not as something we are born with but rather as something acquired—or, more precisely, _earned._ Individuality begins in the realization that it is impossible to escape being influenced by other people and by circumstance. The essence of individuality is vigilance. The following guidelines will help you achieve this:

1. _Treat your first reaction to any person, issue, or situation as tentative._ No matter how appealing it may be, refuse to embrace it until you have examined it.
2. _Decide why you reacted as you did._ Consider whether you borrowed the reaction from someone else—a parent or friend, perhaps, or a celebrity or fictional character on television. If possible, determine what specific experiences conditioned you to react this way.
3. _Think of other possible reactions you might have had to the person, issue, or situation._
4. _Ask yourself whether one of the other reactions is more appropriate than your first reaction._ And when you answer, resist the influence of your conditioning.

To ensure that you will really be an individual and not merely claim to be one, apply these guidelines throughout your work in this book, as well as in your everyday life.



## Chapter 2 What Is Critical Thinking?

When Arthur was in the first grade, the teacher directed the class to “think.” “Now, class,” she said, “I know this problem is a little harder than the ones we’ve been doing, but I’m going to give you a few extra minutes to think about it. Now start thinking.”

It was not the first time Arthur had heard the word used. He’d heard it many times at home, but never quite this way. The teacher seemed to be asking for some special activity, something he should know how to start and stop—like his father’s car. “Vroom-m-m,” he muttered half aloud. Because of his confusion, he was unaware he was making the noise.

“Arthur, please stop making noises and start thinking.”

Embarrassed and not knowing quite what to do, he looked down at his desk. Then, out of the corner of his eye, he noticed that the little girl next to him was staring at the ceiling. “Maybe that’s the way you start thinking,” he guessed. He decided the others had probably learned how to do it last year, that time he was home with the measles. So he stared at the ceiling.

As he progressed through grade school and high school, he heard that same direction hundreds of times. “No, that’s not the answer, you’re not thinking—now _think!_ ” And occasionally he would hear from particularly self-pitying teachers given to muttering to themselves aloud: “What did I do to deserve this? Don’t they teach them anything in the grades anymore? Don’t you people care about ideas? Think, dammit, THINK.”

So Arthur learned to feel somewhat guilty about the whole matter. Obviously, this thinking was an important activity that he’d failed to learn. Maybe he lacked the brain power. But he was resourceful enough. He watched the other students and did what they did. Whenever a teacher started in about thinking, he screwed up his face, furrowed his brow, scratched his head, stroked his chin, stared off into space or up at the ceiling, and repeated silently to himself, “Let’s see now, I’ve got to think about that, think, think—I hope he doesn’t call on me—think.” Though Arthur didn’t know it, that’s just what the other students were saying to themselves.

Your experience may have been similar to Arthur’s. In other words, many people may have simply told you to think without ever explaining what thinking is and what qualities a good thinker has that a poor thinker lacks. If that is the case, you have a lot of company. Extensive, effective training in thinking is the exception rather than the rule. This fact and its unfortunate consequences are suggested by the following comments from accomplished observers of the human condition:

> The most interesting and astounding contradiction in life is to me the constant insistence by nearly all people upon “logic,” “logical reasoning,” “sound reasoning,” on the one hand, and on the other their inability to display it, and their unwillingness to accept it when displayed by others.
>
> Most of our so-called reasoning consists in finding arguments for going on believing as we already do.
>
> Clear thinking is a very rare thing, but even just plain thinking is almost as rare. Most of us most of the time do not think at all. We believe and we feel, but we do not think.
>
> Mental indolence is one of the commonest of human traits.

What is this activity that everyone claims is important but few people have mastered? Thinking is a general term used to cover numerous activities, from daydreaming to reflection and analysis. Here are just some of the synonyms listed in Roget’s Thesaurus for think:

> appreciate consult fancy reason 
> believe contemplate imagine reflect 
> cerebrate deliberate meditate ruminate 
> cogitate digest muse speculate 
> conceive discuss ponder suppose 
> consider dream realize weigh

All of those are just the _names_ that thinking goes under. They really don’t explain it. The fact is, after thousands of years of humans’ experiencing thought and talking and writing about thinking, it remains in many respects one of the great mysteries of our existence. Still, though much is yet to be learned, a great deal is already known.

### Mind, Brain, or Both?

Most modern researchers use the word _mind_ synonymously with _brain,_ as if the physical organ that resides in the human skull were solely responsible for thinking. This practice conveniently presupposes that a problem that has challenged the greatest thinkers for millennia—the relationship between mind and physical matter—was somehow solved when no one was looking. The problem itself and the individuals who spent their lives wrestling with it deserve better.

Neuroscience has provided a number of valuable insights into the cognitive or thinking activities of the brain. It has documented that the left hemisphere of the brain deals mainly with detailed language processing and is associated with analysis and logical thinking, that the right hemisphere deals mainly with sensory images and is associated with in tuition and creative thinking, and that the small bundle of nerves that lies between the hemispheres—the corpus callosum—integrates the various functions.

The research that produced these insights showed that the brain is _necessary_ for thought, but it has not shown that the brain is _sufficient_ for thought. In fact, many philosophers claim it can never show that. They argue that the mind and the brain are demonstrably different. Whereas the brain is a _physical_ entity composed of matter and therefore subject to decay, the mind is a _metaphysical_ entity. Examine brain cells under the most powerful microscope and you will never see an idea or concept— for example, beauty, government, equality, or love—because ideas and concepts are not material entities and so have no physical dimension. Where, then, do these nonmaterial things reside? In the nonmaterial mind.

The late American philosopher William Barrett observed that “history is, fundamentally, the adventure of human consciousness” and “the fundamental history of humankind is the history of mind.” In his view, “one of the supreme ironies of modern history” is the fact that science, which owes its very existence to the human mind, has had the audacity to deny the reality of the mind. As he put it, “the offspring denies the parent.”

The argument over whether the mind is a reality is not the only issue about the mind that has been hotly debated over the centuries. One especially important issue is whether the mind is _passive,_ a blank slate on which experience writes, as John Locke held, or _active,_ a vehicle by which we take the initiative and exercise our free will, as G. W. Leibnitz argued. This book is based on the latter view.